---
title: "Lecture 19  Regression Trees and Classification Trees"
author: "The Great Courses"
output: 
  beamer_presentation: 
    keep_tex: yes
    colortheme: "albatross"
    pandoc_args: [ 
      "--template", "./beamer_template.tex"
    ]
---
```{r setup, echo=FALSE, include=FALSE}
knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
knitr::opts_chunk$set(mysize=TRUE)
knitr::opts_chunk$set(size='\\small')
knitr::opts_chunk$set(fig.height=4)
knitr::opts_chunk$set(fig.width=6)
knitr::opts_chunk$set(dpi = 300)
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
# use of knit_hooks$set for par option to be applied to all the chunks
knitr::knit_hooks$set(mylightplot = function(before, options, envir) {
    if (before) 
      par(col="gray",      # color of lines
          col.axis="gray", # color of axis annotation
          col.lab="gray",  # color of x and y labels
          col.main="gray", # color of the main title
          col.sub="gray",  # color of the subtitles
          col.ticks="gray",# color of tick marks
          fg="lightgray",  # color of axes, boxes
          xpd=NA           #JP# clip to the device region
          )
})
knitr::opts_chunk$set(mylightplot=TRUE)
```


## Motivating Example
- Have you ever gone into a grocery store and used one of those machines that automatically sorts your coins and gives you back cash?

- The way that those machines work is by classifying the coins based on their physical characteristics.

## Motivating Example
- Imagine that we want to create a method for sorting U.S. coins into pennies, nickels, dimes, quarters.  

Is there is a measurement that we could use to differentiate the coins?

## Motivating Example
- One might be, the surface area (or diameter) of the coin.

- We could shake the coins on a surface that has round holes in it that are slightly smaller than the diameter of a quarter

## Motivating Example
Pennies, Nickels and Dimes would fall through to a second surface. On that second surface, there would be smaller holes, this time slightly smaller than a nickel, but large enough for dimes and pennies to fall through.

## Motivating Example
If we continue this process, we'll end up with a layer of quarters at the top, then nickels, then pennies, and dimes at the bottom.

## Motivating Example
We've just constructed a **classification tree**, in this case, a method for classifying a pile of coins.  This is an example of a Decision Tree.

## Decision Trees
In general, a **decision tree** is a graph that uses a branching method to determine all possible outcomes of a decision.

## Tree structure
- The structure of a decision tree is similar to a real tree. There's a root, there are branches, even leaves!  Except, think of it as an upside-down tree.  We start at the root UP TOP and work our way down to the leaves.

## Decision Trees
A **tree** is a graphical representation of a set of rules.

- Tree-based methods involve partitioning the data into a number of smaller regions and using data from those regions for predictions.

## Decision Trees
When displayed graphically, the rules used to partition the predictor space can be summarized in a tree, which we refer to as decision tree methods.

## Recursive Partitioning
Here's how it works.  Suppose we have one predictor variable X and one response, Y.

## Recursive Partitioning
1. Take all of your data.  Consider all possible values of all predictor variables.

## Recursive Partitioning
2. Select the variable, say $X_i = 3$, that produces the greatest "separation" in Y.  $(X_i=3)$ is called a "split".

## Recursive Partitioning
3. If $X_i<3$ then send the data to the "left"; otherwise, send data point to the "right".  Notice that we just do binary splits.

## Recursive Partitioning
**Recursive Partitioning** allows us to partition the space into subdivisions with smaller, more manageable chunks of the data and then fit a simple, local model.

## Trees
- Trees are very easy to explain, and... they give us rules that are easy to interpret and implement.  In fact, (whisper) they are even easier to explain than linear regression!

## Trees
Decision trees more closely mirror the human decision-making approach.

Trees can be displayed graphically and are easily interpreted even by a non-expert.

## Trees
-  Also, trees don't require the assumptions of statistical models and work well even when some data values are missing.

## Regression trees
Trees for continuous outcomes are called **regression trees**, while trees for categorical outcomes are called **classification trees**.

## Regression Tree
**Regression trees**
- A simple, yet powerful way to predict the response variable based on partitioning the predictor variables.  The idea is to split the data into partitions and to fit a constant model of the response variable in each partition. 

## Regression trees
- Regression Trees: use sum of squares _note: NOT "sum of squared errors"_. Find split that produces greatest separation in 
$\sum[y - E(y)]^2$ 

## Regression Tree
So, how do we use data to construct trees that give us useful answers?  There is a large amount of work done in this type of problem. This lecture will give an introductory description.

## Terminology - Nodes
Let's start with some terminology.

**Parent node** (A) - Nodes at the top that split into lower, child nodes ("tree" is upside down)  

## Terminology - Nodes
**Child node** (B, C) - The result of a binary splitting of a parent node. Two child nodes encompass the entire parent node.  

## Terminology - Nodes
**Terminal node** (leaf node)  (D,E,F,G) - The final partitions of the data that contain no splits. This is where the predicted values are.  

## R Example
Let's look at an example in R. 

The `car.test.frame` data frame has 60 rows and 8 columns, giving data on makes of cars taken from the April, 1990 issue of Consumer Reports. 

## R Example
The `rpart` and `tree` libraries in the R tool can be used for classification trees AND for regression trees.

`library(rpart)`
`library(tree)`

## R Example [show only FIVE lines of output, NOT six]
```{r, echo=FALSE}
library(rpart)
library(tree)
data(car.test.frame)
head(car.test.frame[,c(1,2,4,5)])
```


## Summary Statistics
- Price: in US dollars of a standard car model. Range 5866 to 24,760

- Country of origin: France, Germany, Japan , Japan/USA, Korea, Mexico, Sweden and USA

## Summary Statistics
- Reliability: a numeric vector coded 1 to 5. 

- Mileage: fuel consumption miles per gallon. Range 18 to 37.

## Summary Statistics
- Type: Compact, Small, Medium, Large, Sporty, Van

- Weight: weight in pounds. Range 1845 to 3855.

## R Example **SKIP**
```{r, echo=FALSE}
summary(car.test.frame[,1:6])
```

## R Example
The first model we'll consider is one using just one predictor variable of `Weight` to model 'Mileage'.  As long as `Mileage` is a numeric variable, tree assumes we want a regression tree model:

## R Example **SKIP**
```{r, echo=FALSE}
my.tree <- tree(Mileage ~ Weight, car.test.frame)
my.tree
```

## Output
Output of the fitted model shows the partition structure.  The root level (no splits) shows the total number of observations (60), the associated deviance of the response variable, or sum of squares (SSY = 1355), followed by the mean response value for that subset (for the root, this is the overall mean = 24.58 miles per gallon).  

## Output
Subsequent splits refer to these statistics for the associated data subsets, with final nodes (leaves) indicated by asterisks.  For example, the first split, is at the indented rows labeled 2 and 3.  

## Output
Row 2 is split according to weight < 2567.5.  15 observations fell in this category, with a sum of squares equal to 186.9 and a mean of 30.9 miles per gallon.

## Output
And row 3 is split according to weight > 2567.5.  45 observations fell into this category with a sum of squares of 361.2 and a mean of 22.47.  The next level split occurs at both 4, 5 and 6, 7.

## Summary
The `summary` function associated with `tree` lists 

- the formula along with 

- the associated dataset, 

- the number of terminal nodes (or leaves), 

- the residual mean deviance.  

## Summary
(This is the mean square which equals the sum of squares divided by N - #nodes or 60 - 6 to give us 54.)  We also have the 5 number summary of the residuals.  

## R Example **SKIP**
```{r, echo=FALSE}
summary(my.tree)
```

## R Example
To examine the tree, use the plot and text functions:

```{r, eval=FALSE}
plot(my.tree)
text(my.tree)
```

## R Example **SKIP**
```{r, echo=FALSE}
plot(my.tree)
text(my.tree)
```

## R Example
There are 6 terminal nodes produced by 5 splits.  The first partitioned the data into two sets of above and below a weight of 2567.5.  Data values below 2567.5 go to the left.  Data values greater than or equal to 2567.5 fall to the right.

## R Example
The second and third splits occurred at weights of 2280 and 3087.5.  The fourth and fifth splits occurred at weights of 2747.5 and 3637.5.

## R Example
Values at the leaves show the mean Mileage (miles per gallon) associated with each of the 6 final data subsets.  

The height of each branch is proportional to the reduction in sum of squares from each split.  

## Regression Trees - Simple Local Model
For classic regression trees, the local model is the average of all values in the last terminal node.  The flexibility of a tree (its ability to help us correctly classify data) depends... on how many leaves it has.

## Tree size
How large should we grow our tree?

Well, think about it.  A large tree will fragment the data into smaller and smaller samples.  This often leads to a model that **overfits** the sample data and fails if we want to predict.

## Tree size
On the other hand, a small tree might not capture the important relationships among the variables.

## Stopping
So, why did our tree stop at 6 terminal nodes?  

In R, the 'tree' function limits growth by either 

- forcing each terminal node to contain a certain number of "leaves".

## Stopping
- Or, if we add a node to the model, the error has to be reduced by a certain amount.

There are default values in tree that determine the stopping rules associated with having a few remaining samples or splits that add information to the model.

## Stopping Criteria
- We use stopping  rules  control  if  the  tree  growing  process  should  be  stopped  or  not.  Here are a few common stopping rules:

## Stopping Criteria
The node won't be split if:

1. The  size  of  a  node  is  less  than  the user-specified  minimum  node  size. 

## Stopping Criteria
The node won't be split if:

2. The  split  of  a  node  results  in  a  child  node  whose  node  size  is  less  than  the  user-specified minimum child node size value.

## Stopping Criteria
The node won't be split if:

3. The improvement at the next split is smaller than the user-specified minimum improvement.

## Change threshold
Let's see what happens when we change our defaults to a lower threshold.

Here's the code in R.

## R example **SKIP**
`fit <- rpart(Mileage ~ Weight, data=car.test.frame, control = list(minsplit = 10, minbucket = 5,cp = 0.0001), method="anova")`

## R example
Notice we've set:

- minsplit = 10: the minimum number of observations that must be in a node in order for a split to be attempted.

## R example
- minbucket = 5: the minimum number of observations in any terminal leaf node. 

## R example
- cp complexity parameter = 0.0001: Any split that doesn't increase the overall $R^2$ by at least cp at each step is not attempted. 

## Change threshold **SKIP**
```{r, eval=FALSE}
car.tree <- rpart(Mileage ~ Weight, data=car.test.frame, 
                control = list(minsplit = 10, minbucket = 5,                  cp = 0.0001), method="anova") 
plot(car.tree, uniform = TRUE) #JP# uniform vertical tree space 
text(car.tree, digits = 4, use.n = TRUE) 
```

## Change threshold  **SKIP**
```{r, echo=FALSE}
car.tree <- rpart(Mileage ~ Weight, data=car.test.frame, 
                control = list(minsplit = 10, minbucket = 5,                  cp = 0.0001), method="anova") 
plot(car.tree, uniform = TRUE) #JP#
text(car.tree, digits = 4, use.n = TRUE) 
```

## Resulting Tree
The resulting tree here has 7 splits resulting in 8 nodes.  We could change the parameters even more to get a bigger tree.

## R Example - Crosss Val
But, given the tendency for tree models to overfit data, how do we know when we have a good model?  

Tree models use a cross-validation technique that splits the data into `training` set for model fitting and a `testing` set to evaluate how good the fit is.

## Crossvalidation
Cross-validation:
1. Data is randomly divided into a training set and a testing set. (perhaps $80\%$ and $20\%$ or $90\%$ and $10\%$)

## Crossvalidation
2. The tree-growing algorithm gets applied to the training data only. We grow the largest tree we can. (This will likely over-fit the data.) 

## Crossvalidation
3. Prune the tree.  At each pair of leaf nodes with a common parent, calculate the sum of squares error on the **testing data**.  Check to see if the error would be smaller by removing those two nodes and making their parent a leaf. (Go around snipping the children off!) 

## Crossvalidation
We keep doing this until pruning no longer improves the error on the testing data.

The red line is a guide for a cutoff value relative to our complexity parameter.  

## Crossvalidation
The optimal choice of tree size is 5, since this is the first value that falls below the red line.  

Going from 5 to 6 splits doesn't reduce the complexity parameter by a minimum of 0.01.

## R example
```{r, echo=FALSE}
# grow tree
fit <- rpart(Mileage ~ Weight, data=car.test.frame, 
                control = list(minsplit = 10, minbucket = 5,                  cp = 0.0001), method="anova") 
plotcp(fit, col=2,lwd=2) # visualize cross-validation results
```




## R Example
Now, let's create a regression tree that predicts car mileage from price, country, reliability, and car type.

```{r, echo=FALSE}
# grow tree
fit2 <- rpart(Mileage~Price + Country + Reliability + Type,
   method="anova", data=car.test.frame)
```


## R Example
We have an optimal number of splits at 4, since the complexity parameter, cp, doesn't decrease by at least 0.01 from 4 to 5 splits.

## R Example **SKIP**
```{r, echo=FALSE}
plotcp(fit2, col=2, lwd=2) # visualize cross-validation results
```


## R Example
Here's the output of our regression tree.
 First there's a split by Price, then Type of Car, then Price and Types of car once again.

## R Example **SKIP**
```{r, echo=FALSE, out.width='350px'}
# plot tree
plot(fit2, uniform=TRUE,
   main="Regression Tree for Mileage ", margin=0.1) #JP#
text(fit2, use.n=TRUE, all=TRUE, cex=.8)
```


## Prostate Example
Here's another example.  A set of 146 patients with stage C prostate cancer, from a study exploring the prognostic value of a detection technology called flow cytometry.  We would like to model `pgtime`: Time to progression or last follow-up (years) using a regression tree with the other variables.


## Prostate Example
```{r, echo=FALSE}
data(stagec)
head(stagec)
```


## Prostate Example **SKIP**
```{r, echo=FALSE}
data(stagec)
require(survival)
fit <- rpart(Surv(pgtime, pgstat) ~ ., stagec)
plot(fit, uniform = TRUE) #JP#
text(fit,use.n=TRUE, all=TRUE, cex=.8)
```

## Regression or Regression Trees?
Is it better to run a regular regression or a regression tree?  It depends on the problem.  

## Regression or Regression Trees?
If the relationship between the predictors and response is well approximated by a linear model, then linear regression will not only be a better model fit, it will outperform regression tree methods that don't exploit this linear relationship. 

## Regression Tree pros
On the flip side, The regression tree is one of the easiest understood ways to convey a model to a non-statistician.

## Regression Tree pros
Furthermore, regression trees are among the very few methods that have
been developed that are capable of automatically modeling
interactions without becoming computationally and
statistically infeasible.

## Transition to Classification Trees
A classification tree is pretty similar to a regression tree, except it uses a qualitative response rather than a quantitative one. 

## Classification Trees
For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. 

## Classification Trees
We're often interested not only in the class prediction corresponding to a particular terminal node region, but also in the class proportion of observations that fall into the correct region.

## Classification Trees
- Growing a classification tree also uses binary splitting.

- However, classification trees predict **qualitative** instead of a quantitative outcome. 

## Classification Trees
- In other words, unlike regression trees (numerical predictions), classification trees make predictions for a discrete, categorical variable.

- The decision-making, input variables that are used to split the data can be numerical or categorical. 

## Classification Trees
- Outcome is categorical, so we use the MODE of the terminal nodes as the predicted value.

## Classification Trees
- For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. 


## IRIS example
OK, so let's do an example using the IRIS dataset for three species of Iris.

- We want to find a tree that can classify Iris flowers as setosa, versicolor, or virginica, using some measurements as covariables. 

## IRIS example
- The response variable is categorical, so the resulting tree is ... classification tree or regression tree? 

## IRIS Classification Trees **SKIP**
```{r, echo=FALSE}
tree.model <- tree(Species ~ Sepal.Width + Petal.Width, data=iris)
summary(tree.model)
```


## IRIS Classification Trees **SKIP**
```{r, echo=FALSE}
plot(tree.model, uniform = TRUE) #JP# !this doesn't work!
text(tree.model, use.n=TRUE, all=TRUE, cex=.8)
```

## IRIS example
- We have a nice tree, with 4 splits and 5 nodes.  If `Petal.Width` smaller than 0.8, we label the flower setosa.

- And, if `Petal.Width` is greater than 1.7, we correctly classify the flower as Virginica.  It's the in-between values that we have to fine tune!

## IRIS Classification Trees
```{r, echo=FALSE}
library(rpart)
library(tree)
data(iris)
tree.model <- tree(Species ~ Sepal.Width + Petal.Width, data=iris)
summary(tree.model)
```

## IRIS Classification Trees
Of our 5 nodes, our classification tree only misclassifies 5 out of the 150 flowers, for a misclassification rate of around 3 percent.

We can see those 5 points graphically by looking at a scatterplot of our data along with binary splits.

## IRIS Classification Trees
Notice that for Petal Width between 1.35 and 1.75 we incorrectly classify 3 virginica as versicolor and incorrectly classify 2 versicolor as virginica. (There's some overlap in values for a couple of the points which is why you don't see all 5 on the plot)

## IRIS Classification Tree **SKIP**
```{r, echo=FALSE}
library(ggplot2)
library(tree)

gg.partition.tree <- function (tree, label = "yval", ordvars, ...) 
{
    ptXlines <- function(x, v, xrange, xcoord = NULL, ycoord = NULL, 
        tvar, i = 1L) {
        if (v[i] == "<leaf>") {
            y1 <- (xrange[1L] + xrange[3L])/2
            y2 <- (xrange[2L] + xrange[4L])/2
            return(list(xcoord = xcoord, ycoord = c(ycoord, y1, 
                y2), i = i))
        }
        if (v[i] == tvar[1L]) {
            xcoord <- c(xcoord, x[i], xrange[2L], x[i], xrange[4L])
            xr <- xrange
            xr[3L] <- x[i]
            ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                1L)
            xr <- xrange
            xr[1L] <- x[i]
            return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                ll2$i + 1L))
        }
        else if (v[i] == tvar[2L]) {
            xcoord <- c(xcoord, xrange[1L], x[i], xrange[3L], 
                x[i])
            xr <- xrange
            xr[4L] <- x[i]
            ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                1L)
            xr <- xrange
            xr[2L] <- x[i]
            return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                ll2$i + 1L))
        }
        else stop("wrong variable numbers in tree.")
    }
    if (inherits(tree, "singlenode")) 
        stop("cannot plot singlenode tree")
    if (!inherits(tree, "tree")) 
        stop("not legitimate tree")
    frame <- tree$frame
    leaves <- frame$var == "<leaf>"
    var <- unique(as.character(frame$var[!leaves]))
    if (length(var) > 2L || length(var) < 1L) 
        stop("tree can only have one or two predictors")
    nlevels <- sapply(attr(tree, "xlevels"), length)
    if (any(nlevels[var] > 0L)) 
        stop("tree can only have continuous predictors")
    x <- rep(NA, length(leaves))
    x[!leaves] <- as.double(substring(frame$splits[!leaves, "cutleft"], 
        2L, 100L))
    m <- model.frame(tree)
    if (length(var) == 1L) {
        x <- sort(c(range(m[[var]]), x[!leaves]))
        if (is.null(attr(tree, "ylevels"))) 
            y <- frame$yval[leaves]
        else y <- frame$yprob[, 1L]
        y <- c(y, y[length(y)])
        if (add) 
            lines(x, y, type = "s", ...)
        else {
            a <- attributes(attr(m, "terms"))
            yvar <- as.character(a$variables[1 + a$response])
            xo <- m[[yvar]]
            if (is.factor(xo)) 
                ylim <- c(0, 1)
            else ylim <- range(xo)
            plot(x, y, ylab = yvar, xlab = var, type = "s", ylim = ylim, 
                xaxs = "i", ...)
        }
        invisible(list(x = x, y = y))
    }
    else {
        if (!missing(ordvars)) {
            ind <- match(var, ordvars)
            if (any(is.na(ind))) 
                stop("unmatched names in vars")
            var <- ordvars[sort(ind)]
        }
        lab <- frame$yval[leaves]
        if (is.null(frame$yprob)) 
            lab <- format(signif(lab, 3L))
        else if (match(label, attr(tree, "ylevels"), nomatch = 0L)) 
            lab <- format(signif(frame$yprob[leaves, label], 
                3L))
        rx <- range(m[[var[1L]]])
        rx <- rx + c(-0.025, 0.025) * diff(rx)
        rz <- range(m[[var[2L]]])
        rz <- rz + c(-0.025, 0.025) * diff(rz)
        xrange <- c(rx, rz)[c(1, 3, 2, 4)]
        xcoord <- NULL
        ycoord <- NULL
        xy <- ptXlines(x, frame$var, xrange, xcoord, ycoord, 
            var)
        xx <- matrix(xy$xcoord, nrow = 4L)
        yy <- matrix(xy$ycoord, nrow = 2L)
        return(
          list(
            annotate(geom="segment", x=xx[1L, ], y=xx[2L, ], xend=xx[3L, ], yend=xx[4L, ]),
            annotate(geom="text", x=yy[1L, ], y=yy[2L, ], label=as.character(lab), ...)
          )
        )
    }
}


ggplot(iris, 
       aes(Petal.Width, Sepal.Width, color=Species)) + 
  geom_point() +
  gg.partition.tree(tree(Species ~ Sepal.Width + Petal.Width, data=iris), 
                    label="Species", color = "black") 
```


## IRIS Prediction **SKIP**
```{r, echo=FALSE}
library(rpart)
set.seed(101)
alpha     <- 0.7 # percentage of training set
inTrain   <- sample(1:nrow(iris), alpha * nrow(iris))
train.set <- iris[inTrain,]
test.set  <- iris[-inTrain,]
my.prediction <- predict(tree.model, test.set) 
round(my.prediction[1:15,],2)
```

## IRIS Prediction **SKIP**
```{r, echo=FALSE}
my.prediction <- predict(tree.model, test.set) 
round(my.prediction[16:30,],2)
```

## IRIS Prediction
We predict _setosa_ correctly with probability 1 and for most points, we're highly confident in our classification.  At the point labeled 60, we classify with 90 percent confidence that it belongs to _versicolor_ and 10 percent confidence that it belongs to _virginica_.

## IRIS Prediction
But look at point 107.  We're much less confident in our classification...only 40 percent confident that it belongs to _versicolor_ and 60 persent confident that it belongs to _virginica_. Here's where those mis-classifications take place!

## Summary
Classification Trees and Regression Trees are an easily understandable and transparent method for predicting or classifying new records

## Pitfalls **NOT Camera 2**
- However, they aren't without their PITFALLS!

- In fact, they may not perform well when you have structure in your data that isn't well-captured by horizontal or vertical splits. 

## Pitfalls **SKIP**
```{r, echo=FALSE}
library(SemiPar)
data(onions)
attach(onions)
points.cols <- c("red","blue")
plot(dens,yield,col=points.cols[location+1],pch=16,main="Onion density in 2 locations")
legend(100,250,c("Location 1","Location 2"), col=points.cols,pch=rep(16,2))
```

## Pitfalls
- Notice here in the plot of onion growth density at two different locations, while there's separation in the data, horizontal and vertical splits won't help us classify the data very well.

- Next time, we'll see a technique called **polynomial regression** that can better classify this data.

## Pitfalls
Also, regression trees can be highly unstable.  Trees tend to have high variance because a very small change in the data can produce a very different series of splits.

## Pitfalls **Camera 2**
And, since trees don't make any assumptions about the data structure, they usually require larger samples. Lastly, there's no way to capture interactions between variables.

## Pitfalls **Camera 2**
- For interaction among variables, we can revert back to the ever-handy, multiple linear regression.   

- Or.. try a new method!  (like polynomial regression! ... or step function!)  More on those, in the next lecture!  [END]

<!-- ## IRIS example -->

<!-- ```{r, fig.height=2.5, fig.width=3} -->
<!-- library(rpart) -->
<!-- library(rpart.plot) -->
<!-- tree <- rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris, method = "class") -->
<!-- rpart.plot(tree) -->
<!-- ``` -->

<!-- ## IRIS Example -->
<!-- ```{r, echo=FALSE} -->
<!-- library(party) -->
<!-- model2<-ctree(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=iris) -->
<!-- plot(model2) -->
<!-- ``` -->

<!-- ## Misclassification Rate -->
<!-- ```{r} -->
<!-- library(tree) -->
<!-- ir.tr <- tree(Species ~., iris) -->
<!-- misclass.tree(ir.tr) -->
<!-- misclass.tree(ir.tr, detail=TRUE) -->
<!-- ``` -->

<!-- ## IRIS Classification Trees -->
<!-- ```{r, echo=FALSE} -->
<!-- # Point prediction -->
<!-- # Let's translate the probability output to categorical output -->
<!-- maxidx <- function(arr) { -->
<!--     return(which(arr == max(arr))) -->
<!-- } -->
<!-- idx <- apply(my.prediction, c(1), maxidx) -->
<!-- prediction <- c('setosa', 'versicolor', 'virginica')[idx] -->
<!-- table(prediction, test.set$Species) -->
<!-- ``` -->


<!-- ## IRIS Classification Trees -->
<!-- ```{r, echo=FALSE} -->
<!-- # Another way to show the data: -->
<!-- plot(iris$Petal.Width, iris$Sepal.Width, pch=19, col=as.numeric(iris$Species)) -->
<!-- partition.tree(tree.model, label="Species", add=TRUE) -->
<!-- legend("topright",legend=unique(iris$Species), col=unique(as.numeric(iris$Species)), pch=19) -->
<!-- ``` -->


<!-- ## IRIS Classification Trees -->
<!-- ```{r, echo=FALSE} -->
<!-- pruned.tree <- prune.tree(tree.model, best=4) -->
<!-- plot(pruned.tree) -->
<!-- text(pruned.tree) -->
<!-- ``` -->


<!-- ## IRIS Classification Trees -->
<!-- ```{r, echo=FALSE} -->
<!-- pruned.prediction <- predict(pruned.tree, test.set, type="class") # give the predicted class -->
<!-- table(pruned.prediction, test.set$Species) -->
<!-- ``` -->


<!-- ## IRIS Classification Trees -->
<!-- ```{r, echo=FALSE} -->
<!-- rpart.tree <- rpart(Species ~ ., data=train.set) -->
<!-- plot(rpart.tree, uniform=TRUE, branch=0.6, margin=0.05) -->
<!-- text(rpart.tree, all=TRUE, use.n=TRUE) -->
<!-- title("Training Set's Classification Tree") -->
<!-- predictions <- predict(rpart.tree, test.set, type="class") -->
<!-- table(test.set$Species, predictions) -->
<!-- ``` -->

